<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Garrett Swan's website</title>

  <link rel="stylesheet" href="hover_test1.css">
</head>
<body>

<h1 id="MainHeader"> Garrett Swan
  <nav>
    <ul id="WebsiteLists">
      <li id="Link1_list" class="Links_list">
        <a href="https://garrettswan.github.io/index.html" class="Links"> Home </a></li>
      <li id="Link2_list" class="Links_list">
        <a href="https://garrettswan.github.io/research.html" class="Links"> Research </a></li>
      <li id="Link3_list" class="Links_list">
        <a href="https://garrettswan.github.io/teaching_mentoring.html" class="Links"> Teaching/Mentoring </a></li>
      <li id="Link4_list" class="Links_list">
        <a href="pdfs/GarrettSwan_CV_11-09-2020.pdf" target="_blank" class="Links"> CV </a></li>
      <li id="Link5_list" class="Links_list">
        <a href="https://osf.io/m24zh/" target="_blank" class="Links"> Code </a></li>
      <li id="Link6_list" class="Links_list">
        <a href="https://garrettswan.github.io/fun_stuff.html" class="Links"> Misc. Fun Stuff </a></li>
    </ul>
  </nav>
</h1>

<main>
  <article>
    <section id="Research1">
      <h1>When driving, how do move our eyes to detect potential hazards?</h1>
      <p>My research using driving simulation has focused evaluating how the people
        move their eyes while driving to detect hazards. Specifically, this research has evaluated
        how individuals with vision impairment (e.g., simulated central vision
        loss, real central vision loss, and real peripheral visual field loss)
        compensate for their visual field loss. I have also developed a pipeline
        that automates processing of driving simulator and eye and head tracking data.
        Furthermore, I have used the driving simulator to investigate change blindness to hazards.</p>

        <p class="hover_img">
           <a href="pdfs/SwanGoldsteinSavageZhangAhmadiBowers_inpress.pdf" target="_blank" class="Links">Show Image<span><img src="Owl_picture4.jpg" alt="image" height="100" /></span></a>
        </p>
        <p><a href="pdfs/SwanGoldsteinSavageZhangAhmadiBowers_inpress.pdf" target="_blank" class="Links"> Automatic Processing of Gaze Movements to Quantify Gaze Scanning Behaviors in a Driving Simulator </a></p>
        <p><a href="pdfs/Savageetal2020TRFNONMCPDF.pdf" target="_blank" class="Links"> The Effects of Age on the Contributions of Head and Eye Movements to Scanning Behavior at Intersections</a></p>
        <p><a href="pdfs/SwanShahinAlbertHerrmannBowers2019.pdf" target="_blank" class="Links"> The Effects of Simulated Acuity and Contrast Sensitivity Impairments on Detection of Pedestrian Hazards in a Driving Simulator </a></p>

      <p></p>
    </section>
    <section id="Research2">
      <h1>How much does relevance influence our memories?</h1>
      <p>When looking at objects, do we only remember the parts of the object that
        are relevant to us or do we remember everything about those objects? One
        of the difficulties of this research question is that simply asking about
        what they remember influences what they focus on in subsequent trials. I
        have used surprise trial methodologies to probe how much people remember about
        task irrelevant portions of an object, which has important implications in
        eye witness testimony</p>

        <p><a href="pdfs/SwanWybleChen2017.pdf" target="_blank" class="Links"> Working Memory Representations Persist in the Face of Unexpected Task Alterations</a></p>
        <p><a href="pdfs/ChenSwanWyble2016.pdf" target="_blank" class="Links"> Prolonged Focal Attention Without Binding: Tracking a Ball for Half a Minute Without Remembering its Color </a></p>
        <p><a href="pdfs/SwanCollinsWyble2016.pdf" target="_blank" class="Links"> Memory for a Single Object has Differently Variable Precisions for Relevant and Irrelevant Features </a></p>

    </section>

    <section id="Research3">
      <h1>How does visual working memory work?</h1>
      <p>My research  focused on the testing and creation of a computational
        simulation of how visual information is bound to form a memory representation
        in visual working memory. This model, called the Binding Pool model,
        simulated encoding as the conjunction of the visual features that compose
        an object (e.g., color, size) and a token, the ability to individuate an
        object in memory from other objects in memory. The Binding Pool model
        provided a mechanism for simulating the trade-off between the number of
        objects maintained in memory and the quality of each memory representation</p>

        <p><a href="pdfs/SwanWyble2014.pdf" target="_blank" class="Links"> The Binding Pool: A Model of Shared Neural Resources for Distinct Items in Visual Working Memory </a></p>
        <p><a href="pdfs/WybleSwanCallahan-Flintof2016.pdf" target="_blank" class="Links"> Measuring Visual Memory in Its Native Format </a></p>
        <p><a href="pdfs/RajsicSwanWilsonPratt2016.pdf" target="_blank" class="Links"> Accessibility Limits Recall From Visual Working Memory </a></p>
        <p><a href="pdfs/AhmadSwanBowmanWybleNobreShapiroMcnab2017.pdf" target="_blank" class="Links"> Competitive Interactions Affect Working Memory Performance for Both Simultaneous and Sequential Stimulus Presentation </a></p>

    </section>

    <section id="Research4">
      <h1>How do we deploy our attention in space and time when there are multiple targets?</h1>
      <p>There are limitations in how visual attention can be deployed to multiple targets
        that appear close in time (e.g., attentional blink) and close in spatial proximity
        (e.g., localized attentional interference). However, these effects are typically evaluated
        with either space or time held constant. Here, we varied space and time to see if
        these effects would be present in more uncertain conditions.</p>

      <p><a href="pdfs/WybleSwan2015.pdf" target="_blank" class="Links"> Mapping the Spatiotemporal Dynamics of Interference Between Two Visual Targets </a></p>

    </section>
  </article>

  <footer id="FooterID">
    <p id="FooterEmails"> gsp.swan@gmail.com &nbsp;&nbsp;&nbsp; or &nbsp;&nbsp;&nbsp; garrettswan@meei.harvard.edu &nbsp;&nbsp;&nbsp; or &nbsp;&nbsp;&nbsp; gswan@ucsd.edu</p>
    <p><a href="https://scholar.google.com/citations?user=vvq9ZGIAAAAJ&hl=en" target="_blank" class="Links" id="GoogleScholarLink">Google Scholar</a> &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/garrett-swan-phd-87627347/" target="_blank" class="Links" id="LinkedInLink">LinkedIn</a> &nbsp;&nbsp;&nbsp;
    <a href="https://osf.io/m24zh/" target="_blank" class="Links" id="GoogleScholarLink">Open Science Framework</a> &nbsp;&nbsp;&nbsp;
    <a href="https://www.ncbi.nlm.nih.gov/myncbi/1p5vZLTOnw55q/bibliography/public/" target="_blank" class="Links" id="GoogleScholarLink">NCBI Bibliography</a>&nbsp;&nbsp;&nbsp;
    <a href="https://github.com/garrettswan" target="_blank" class="Links" id="GoogleScholarLink">Github</a></p>
  </footer>
</main>

</body>
</html>

</html>
